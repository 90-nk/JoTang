# <p align="middle"> 深度学习笔记
## 一、独热码（One-Hot）
百度百科的解释：  
"  
一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。  
"  
*我的理解：“  
不使用123456的顺序进行编码，而采用000001、000010、000100、001000来区分第123个数据。这样的好处在于各个数据的序号之间不存在6>5>4>3这样的大小关系，并且不同数据之间只有1的位置不同，便于区分  
”*  
## 二、向量化（Vectorization）  
Non-vectorized  
> z=0  
> for i in rang (n-x):  
>   z-=w[i]*x[i]
> z+=b

Vectorized
> z=np.dot.(w,x)+b

总体而言，向量化的过程是一个加速代码运行的过程。由time模块得到的向量化前后的代码运行所用时间反映出，向量化的前后，代码的运行速度的差大概在300倍左右。1.5ms与450ms的差距可能很小，但15分钟与450分钟之间的时间差就大了。  
*自己的理解：
向量这一概念本身以我现在的数学能力来说有点难以理解，但是结合百度百科进行了部分理解过程之后，发现向量这一概念有点像张量。在机器学习的过程中的张量增多，会像是从X-Y二维图象增多一根轴变成X-Y-Z三维图象这样，更多的张量似乎难以被人类理解了，但是是计算机进行衡量过程的可用标准。向量则比较像是n维空间内用来确定某一点的基本量，比如二维需要不共线的两个向量，三维需要不共面的三个向量。但是其实，这样的理解并没有让我理解到向量化与该程序之间的密切关联。可能引入了矩阵，矩阵又和向量关系比较密切吧？*  
## 三、神经网络概览
### （一）神经元：  
神经元接受前一层的输入，经过处理会有一个输出。  
*z=δ(w1x1+w2x2+w3x3+...b)*  
δ为激活函数，w为权重，x为输入，b为偏移项。  
总体而言，一个神经元会有下面五个部分组成：  
> 1.输入：x1,x2,x2...  
> 2.权重：w1,w2,w3...  
> 3.偏移项
> 4.激活函数：δ（Sigmoid,Tanh,ReLU,Leaky ReLU）  

*Sigmoid:
### （二）输入层:  
用于数据以供给给神经网络进行学习。  
### （三）隐藏层：  
除输入层和输出层以外的其他各层。  

**注意：** 隐藏层不直接接受外界的信号，也不直接向外界发送信号。  

### （四）输出层：
## 四、卷积神经网络
### （一）卷积：
1.卷积在卷积神经网络中的主要作用是提取图片的特征，同时保留原来图片中各个像素的相对位置关系。
2.存在一个卷积核（滤波器）对原始图像矩阵进行处理，对滤波器矩阵设置不同的值可以实现如边缘检测、锐化以及模糊操作等不同的操作。**使用越多的滤波器，可以提取到越多的图像特征，神经网络也有更好的性能。**  
3.卷积之后得到的矩阵，被称为特征图。
##  五、杂
### （一）梯度